{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Generate K Means Examples\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_data = 1000\n",
    "seed = 1\n",
    "n_clusters = 4\n",
    "\n",
    "# Generate random Gaussian blobs and run k-means\n",
    "blobs, blob_labels = make_blobs(n_samples = n_data, n_features=2, centers = n_centers, random_state = seed)\n",
    "clusters_blob = KMeans(n_clusters = n_centers, random_state = seed).fit_predict(blobs)\n",
    "\n",
    "# Generate data uniformly at random and run k-means\n",
    "uniform = np.random.rand(n_data, 2)\n",
    "clusters_uniform = KMeans(n_clusters = n_clusters, random_state = seed).fit_predict(uniform)\n",
    "\n",
    "# Matplotlib incantations for visualizing\n",
    "figure = plt.figure()\n",
    "plot.subplot(221)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], c = blob_labels, cmap = 'gist_rainbow')\n",
    "plt.title(\"Four randomly generated blobs\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plot.subplot(222)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], c = clusters_blob, cmap = 'gist_rainbow')\n",
    "plt.title(\"Clusters found via K Means\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plot.subplot(223)\n",
    "plt.scatter(uniform[:, 0], uniform[:, 1])\n",
    "plt.title(\"1000 randomly generated points\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plot.subplot(224)\n",
    "plt.scatter(uniform[:, 0], uniform[:, 1], c = clusters_uniform, cmap = 'gist_rainbow')\n",
    "plt.title(\"Clusters found via K Means\", fontsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on the Swiss Roll\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "# Generate a noisy Swiss roll dataset\n",
    "X, color = datasets.sample_generator.make_swiss_roll(n_samples=1500)\n",
    "\n",
    "# Approximate the data with 100 k-means clusters\n",
    "clusters_swiss_roll = KMeans(n_clusters=100, random_state=1).fit_predict(X)\n",
    "\n",
    "# Plot the dataset with k-means cluster IDs as the color\n",
    "fig2 = plt.figure()\n",
    "ax = fig2.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[;, 1], X[:, 2], c=clusters_swiss_roll, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Featurizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class KMeansFeaturizer:\n",
    "    \"\"\" Transforms numeric data into k-means cluster memberships.\n",
    "        This transformer runs k-means on the input data and converts each data point into the ID of the closest cluster.\n",
    "        If a target var is present, it is scaled and included as input to k-means in order to derive clusters that\n",
    "        obey the classification boundary as well as group similar points together.\"\"\"\n",
    "    \n",
    "    def __init__(self, k=100, target_scale=5.0, random_state=None):\n",
    "        self.k = k\n",
    "        self.target_scale = target_scale\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Runs k-means on the input data and finds centroids\"\"\"\n",
    "        if y is None:\n",
    "            # No target var, do simple k-means\n",
    "            km_model = KMeans(n_clusters=self.k, n_init=20, random_state=self.random_state)\n",
    "            km_model.fit(X)\n",
    "            self.km_model = km_model\n",
    "            self.cluster_centers_ = km_model.cluster_centers_\n",
    "            return self\n",
    "        # There is a target info. Apply appropriate scaling and include it in the input data to k-means\n",
    "        data_with_target = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n",
    "        \n",
    "        # Build a pre-training k-means model on data and target\n",
    "        km_model_pretrain = KMeans(n_clusers=self.k, n_init=20, random_state=self.random_state)\n",
    "        km_model_pretrain.fit(data_with_target)\n",
    "        \n",
    "        # Run k-means a second time to get the clusters in the original space\n",
    "        # without target info. Initialize using centroids found in pre-training.\n",
    "        # Go through a single iteration of cluster assignment and centroid recomputation.\n",
    "        km_model = KMeans(n_clusters=self.k, init=km_model_pretrain.cluster_centers_[:, :2], n_init=1, max_iter=1)\n",
    "        km_model.fit(X)\n",
    "        \n",
    "        self.km_model = km_model\n",
    "        self.cluster_centers_ = km_model.cluster_centers_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Outputs the closest cluster ID for each input data point.\"\"\"\n",
    "        clusters = self.km_model.predict(X)\n",
    "        return clusters[:, np.newaxis]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means featurization with and without target hints\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "training_data, training_labels = make_moons(n_samples=2000, noise=0.2)\n",
    "kmf_hint = KMeansFeaturizer(k=100, target_scale=10).fit(training_data, training_labels)\n",
    "kmf_no_hint = KMeansFeaturizer(k=100, target_scale=0).fit(training_data)\n",
    "\n",
    "def kmeans_voronoi_plots(X, y, cluster_centers, ax):\n",
    "    \"\"\"Plots the Voronoi diagram of the k-means clusters overlaid with the data\"\"\"\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', alpha=0.2)\n",
    "    vor = Voronoi(cluster_centers)\n",
    "    voronoi_plot_2d(vor, ax=ax, show_vertices=False, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification with k-means cluster features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "training_data, training_labels = make_moons(n_samples=2000, noise=0.2)\n",
    "# Generate some test data from the same distribution as training data\n",
    "test_data, test_labels = make_moons(n_samples=2000, noise=0.3)\n",
    "\n",
    "# Use the k-means featurizer to generate cluster features\n",
    "training_cluster_features = kmf_hint.transform(training_data)\n",
    "test_cluster_features = kmf_hint.transform(test_data)\n",
    "\n",
    "# Form new input features with cluster features\n",
    "training_with_cluster = scipy.sparse.hstack((training_data, training_cluster_features))\n",
    "test_with_cluster = scipy.sparse.hstack((test_data, test_cluster_features))\n",
    "\n",
    "# Build the classifiers\n",
    "lr_cluster = LogisticRegression(random_state=seed).fit(training_with_cluster, training_labels)\n",
    "classifier_names = ['LR', 'kNN', 'RBF SVM', 'Random Forest', 'Boosted Trees']\n",
    "classifiers = [LogisticRegression(random_state=seed), KNeighborsClassifier(5), SVC(gamma=2, C=1),\n",
    "              RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "              GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=5)]\n",
    "\n",
    "for model in classifiers:\n",
    "    model.fit(training_data, training_labels)\n",
    "    \n",
    "# Helper function to evaluate classifier performance using ROC\n",
    "def test_roc(model, data, labels):\n",
    "    if(hasattr(model, \"decision_function\")):\n",
    "        predictions = model.decision_function(data)\n",
    "    else:\n",
    "        predictions = model.predict_proba(data)[:, 1]\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    return fpr, tpr\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "fpr_cluster, tpr_cluster = test_roc(lr_cluster, test_with_cluster, test_labels)\n",
    "plt.plot(fpr_cluster, tpr_cluster, 'r-', label = 'LR with k-means')\n",
    "\n",
    "for i, model in enumerate(classifiers):\n",
    "    fpr, tpr = test_roc(model, test_data, test_labels)\n",
    "    plt.plot(fpr, tpr, label = classifier_names[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
