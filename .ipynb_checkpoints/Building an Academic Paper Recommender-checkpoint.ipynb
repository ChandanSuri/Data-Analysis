{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Based Collaboratibe Filtering\n",
    "# Using Microsoft Academic Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st Part : Data Import, Cleaning and Feature Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Approach:\n",
    "# Import + filter data\n",
    "import pandas as pd\n",
    "model_df = pd.read_json('data/mag_papers_0/mag_subset20K.txt', lines = True)\n",
    "print(model_df.shape)\n",
    "print(model_df.columns)\n",
    "\n",
    "# Filter out non-Ensglish articles and focus on a few variables\n",
    "model_df = model_df[model_df.lang == 'en'].drop_duplicates(subset='title', keep='first').drop(['doc_type', 'doi', 'id', 'issue'\n",
    "                                                                                              , 'lang', 'n_citation', 'page_end',\n",
    "                                                                                              'page_start', 'publisher', \n",
    "                                                                                              'references', 'url', 'venue', \n",
    "                                                                                              'volume'], axis=1)\n",
    "print(model_df.shape)\n",
    "\n",
    "# Transforming Feature arrays\n",
    "\n",
    "# Collaborative filtering stage 1: Build Item feature matrix\n",
    "unique_fos = sorted(list({feature for paper_row in model_df.fos.fillna('0') for feature in paper_row}))\n",
    "\n",
    "unique_year = sorted(model_df['year'].astype('str').unique())\n",
    "\n",
    "def feature_array(x, var, unique_array):\n",
    "    row_dict = {}\n",
    "    for i in x.index:\n",
    "        var_dict = {}\n",
    "        for j in range(len(unique_array)):\n",
    "            if type(x[i]) is list:\n",
    "                if unique_array[j] in x[i]:\n",
    "                    var_dict.update({var + '_' + unique_array[j]:1})\n",
    "                else:\n",
    "                    var_dict.update({var + '_' + unique_array[j]:0})\n",
    "            else:\n",
    "                if unique_array[j] == str(x[i]):\n",
    "                    var_dict.update({var + '_' + unique_array[j]:1})\n",
    "                else:\n",
    "                    var_dict.update({var + '_' + unique_array[j]:0})\n",
    "        row_dict.update({i: var_dict})\n",
    "    feature_df = pd.DataFrame.from_dict(row_dict, dtype='str').T\n",
    "    return feature_df\n",
    "\n",
    "year_features = feature_array(model_df['year'], unique_year)\n",
    "fos_features = feature_array(model_df['fos'], unique_fos)\n",
    "first_features = fos_features.join(year_features).T\n",
    "\n",
    "from sys import getsizeof\n",
    "print('Size of first feature array: ', getsizeof(first_features))\n",
    "\n",
    "# Collaborative filtering stage 2: Search for similar items\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def item_collab_filter(features_df):\n",
    "    item_similarities = pd.DataFrame(index=features_df.columns, columns=features_df.columns)\n",
    "    for i in features_df.columns:\n",
    "        for j in features_df.columns:\n",
    "            item_similarities.loc[i][j] = 1- cosine(features_df[i], features_df[j])\n",
    "    return item_similarities\n",
    "first_items = item_collab_filter(first_features.loc[:, 0:1000])\n",
    "\n",
    "# Heatmap for paper recommendations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "ax = sns.heatmap(first_items.fillna(0), vmin=0, vmax=1, cmap = \"YlGnBu\", xticklabels=250, yticklabels=250)\n",
    "ax.tick_params(labelsize=12)\n",
    "# Though not much success after this, but we have many more features to work at\n",
    "\n",
    "def paper_recommender(paper_ix, items_df):\n",
    "    print('Based on the paper: \\n Index = ', paper_ix)\n",
    "    print(model_df.iloc[paper_ix])\n",
    "    top_results = items_df.loc[paper_ix].sort_values(ascending=False).head(4)\n",
    "    print('\\nTop three results: ')\n",
    "    order = 1\n",
    "    for i in top_results.index.tolist()[-3:]:\n",
    "        print(order, '. Paper index = ', i)\n",
    "        print('Similarity Score: ', top_results[i])\n",
    "        print(model_df.iloc[i], '\\n')\n",
    "        if order <5:\n",
    "            order += 1\n",
    "\n",
    "paper_recommender(2, first_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second approach: Take 2\n",
    "# Fixed-width binning + dummy coding (part 1)\n",
    "print(\"Year Spread: \", model_df['year'].min(), \" - \", model_df['year'].max())\n",
    "print(\"Quantile Spread:\\n\", model_df['year'].quantile([0.25, 0.5, 0.75]))\n",
    "\n",
    "# Plot years to see the distribution\n",
    "fig, ax = plt.subplots()\n",
    "model_df['year'].hist(ax=ax, bins=model_df['year'].max() - model_df['year'].min())\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_xlabel('Year Count', fontsize=12)\n",
    "ax.set_ylabel('Occurence', fontsize=12)\n",
    "# Graph shows that this is an excellent candidate for binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed-width binning + dummy coding (part 2)\n",
    "# binning here (by 10 years) reduces the year feature space from 156 to 19\n",
    "bins = int(round((model_df['year'].max() - model_df['year'].min())/10))\n",
    "temp_df = pd.DataFrame(index=model_df.index)\n",
    "temp_df['yearBinned'] = pd.cut(model_df['year'].tolist(), bins, precision=0)\n",
    "X_yrs = pd.get_dummies(temp_df['yearBinned'])\n",
    "print(X_yrs.columns.categories)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "X_yrs.sum().plot.bar(ax=ax)\n",
    "ax.tick_params(labelsize=8)\n",
    "ax.set_xlabel('Binned Years', fontsize=12)\n",
    "ax.set_ylabel('Counts', fontsize=12)\n",
    "\n",
    "# Now going to the fields of study field\n",
    "# Converting bag-of-phrases pd.Series ti Numpy Sparse array\n",
    "X_fos = fos_features.values\n",
    "\n",
    "# We can see how this will make a difference in the future by looking at the size of each\n",
    "print('Our panda Series, in bytes: ', getsizeof(fos_features))\n",
    "print('Our hashed numpy array, in bytes: ', getsizeof(X_fos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering stages 1+2: Build item feature matrix, search for similar items\n",
    "second_features = np.append(X_fos, X_yrs, axis = 1)\n",
    "print(\"The power of feature engineering saves us, in bytes: \", getsizeof(first_features) - getsizeof(second_features))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def piped_collab_filter(features_matrix, index, top_n):\n",
    "    item_similarities = 1 - cosine_similarity(features_matrix[index:index+1], features_matrix).flatten()\n",
    "    related_indices = [i for i in item_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, item_similarities[index]) for index in related_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based collaborative filtering recommendations: Take 2\n",
    "def paper_recommender(items_df, paper_ix, top_n):\n",
    "    if paper_ix in model_df.index:\n",
    "        print('Based on the paper: ')\n",
    "        print('Paper index = ', model_df.loc[paper_ix].name)\n",
    "        print('Title: ', model_df.loc[paper_ix]['title'])\n",
    "        print('FOS: ', model_df.loc[paper_ix]['fos'])\n",
    "        print('Year: ', model_df.loc[paper_ix]['year'])\n",
    "        print('Abstract: ', model_df.loc[paper_ix]['abstract'])\n",
    "        print('Authors: ', model_df.loc[paper_ix]['authors'], '\\n')\n",
    "        # Define the location index for the DataFrame index requested\n",
    "        array_ix = model_df.index.get_loc(paper_ix)\n",
    "        top_results = piped_collab_filter(items_df, array_ix, top_n)\n",
    "        print('\\nTop ', top_n, 'results: ')\n",
    "        order = 1\n",
    "        for i in range(len(top_results)):\n",
    "            print(order, '. Paper index = ', model_df.iloc[top_results[i][0]].name)\n",
    "            print('Similarity score: ', top_results[i][1])\n",
    "            print('Title: ', model_df.iloc[top_results[i][0]]['title'])\n",
    "            print('FOS: ', model_df.iloc[top_results[i][0]]['fos'])\n",
    "            print('Year: ', model_df.iloc[top_results[i][0]]['year'])\n",
    "            print('Abstract: ', model_df.iloc[top_results[i][0]]['abstract'])\n",
    "            print('Authors: ', model_df.iloc[top_results[i][0]]['authors'], '\\n')\n",
    "            if order < top_n: \n",
    "                order += 1\n",
    "            else:\n",
    "                print('Whoops! Choose another paper. Try something from here: \\n', model_df.index[100:200])\n",
    "\n",
    "paper_recommender(second_features, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic Paper Recommender: Take 3\n",
    "# We will take now abstract -> tf-idf as well as authors into account, increasing features to find better similarity measure\n",
    "# Stopwords + tf-idf\n",
    "\n",
    "# need to fill Nan for sklearn use in future\n",
    "filled_df = model_df.fillna('None')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "X_abstract = vectorizer.fit_transform(filled_df['abstract'])\n",
    "third_features = np.append(second_features, X_abstract.toarray(), axis=1)\n",
    "\n",
    "# For authors\n",
    "# One hot encoding using scikit-learn's DictVectorizer\n",
    "authors_list = []\n",
    "\n",
    "for row in filled_df.authors.itertuples():\n",
    "    # Create a dictionary from each Series index\n",
    "    if type(row.authors) is str:\n",
    "        y = {'None': row.Index}\n",
    "    if type(row.authors) is list:\n",
    "        y = dict.fromkeys(row.authors[0].values(), row.Index)\n",
    "    authors_list.append(y)\n",
    "print(authors_list[0:5])\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = authors_list\n",
    "X_authors = v.fit_transform(D)\n",
    "fourth_features = np.append(third_features, X_authors, axis=1)\n",
    "\n",
    "# Seeing the recommender now\n",
    "paper_recommender(fourth_features, 2, 3)\n",
    "# This is the best till here, more possibilities after this..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
