{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4afba7cc7a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualizing Business Review Counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Yelp Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# 2-2\n",
    "# Visualizing Business Review Counts\n",
    "# Yelp Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_file = open('yelp_academic_dataset_business.json')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_file.readlines()])\n",
    "biz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the review counts\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "biz_df['review_count'].hist(ax=ax, bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(labelsize = 14)\n",
    "ax.set_xlabel('Review Count', fontsize=14)\n",
    "ax.set_ylabel('Occurrence', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "# Quantizing Counts with fixed-width bins\n",
    "import numpy as np\n",
    "# Generate 20 random integers uniformly between 0 and 99\n",
    "small_counts = np.random.randint(0, 100, 20)\n",
    "print(small_counts)\n",
    "# Map to evenly spaced bins 0-9 by division\n",
    "np.floor_divide(small_counts, 10)\n",
    "# An array of counts that span several magnitudes\n",
    "large_counts = []\n",
    "# Map to exponential-width bins via the log function\n",
    "np.floor(np.log10(large_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-4\n",
    "# Computing deciles of Yelp business review counts\n",
    "deciles= biz_df['review_count'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "print(deciles)\n",
    "# Visualize the deciles on the histogram\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "biz_df['review_count'].hist(ax=ax, bins=100)\n",
    "for pos in deciles:\n",
    "    handle= plt.axvline(pos, color = 'r')\n",
    "ax.legend([handle], ['deciles'], fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('Review Count', fontsize=14)\n",
    "ax.set_ylabel('Occurrence', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-5\n",
    "# Binning counts by quantiles\n",
    "# Map the counts to quartiles\n",
    "pd.qcut(large_counts, 4, labels = False)\n",
    "# Compute the quantiles themselves\n",
    "large_counts_series = pd.Series(large_counts)\n",
    "large_counts_series.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-6\n",
    "# Log Transformation\n",
    "# Visualizing the distribution of review counts before and after log transform\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "biz_df['review_count'].hist(ax=ax1, bins=100)\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_xlabel('review_count', fontsize=14)\n",
    "ax1.set_ylabel('Occurrence', fontsize=14)\n",
    "\n",
    "biz_df['log_review_count'].hist(ax=ax2, bins=100)\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_xlabel('log10(review_count)', fontsize=14)\n",
    "ax2.set_ylabel('Occurrence', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-8\n",
    "# Using log transformed Yelp review counts to predict average business rating\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# we add 1 to the raw count to prevent the logarithm from exploding into -ve infinity in case count = 0\n",
    "biz_df['log_review_count'] = np.log10(biz_df['review_count'] + 1)\n",
    "\n",
    "# Train linear regression models to predict the avg start rating of a business,\n",
    "# using the review_count feature with and without log transformation.\n",
    "# Compare the 10-fold cross validation score of the 2 models\n",
    "m_orig = linear_model.LinearRegression()\n",
    "scores_orig = cross_val_score(m_orig, biz_df['review_count'], ..., biz_df['stars'], cv=10)\n",
    "\n",
    "m_log = linear_model.LinearRegression()\n",
    "scores_log = cross_val_score(m_log, biz_df['log_review_count'], ..., biz_df['stars'], cv=10)\n",
    "\n",
    "print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" % (scores_orig.mean(), scores_orig.std() * 2))\n",
    "print(\"R-squared score with log transform: %0.5f (+/- % 0.5f)\" % (scores_log.mean(), scores_log.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-9\n",
    "# Using log transformed word counts in the Online News popularity dataset to predict article popularity\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv', delimiter = ', ')\n",
    "# n_tokens_count represents the number of words in the news article\n",
    "df['log_n_tokens_content'] = np.log10(df['n_tokens_content'] + 1)\n",
    "\n",
    "# Train two linear regression models to predict the number of shares of an article, on using the original feature and,\n",
    "# the other one is the log transformed version.\n",
    "m_orig = linear_model.LinearRegression()\n",
    "scores_orig = cross_val_score(m_orig, df[['n_tokens_content']], ..., df['shares'], cv=10)\n",
    "\n",
    "m_log = linear_model.LinearRegression()\n",
    "scores_log = cross_val_score(m_log, df[['log_n_tokens_content']], ..., df['shares'], cv=10)\n",
    "\n",
    "print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" % (scores_orig.mean(), scores_orig.std() * 2))\n",
    "print(\"R-squared score with log transform: %0.5f (+/- % 0.5f)\" % (scores_log.mean(), scores_log.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-10\n",
    "# Visualizing the correlation between input and output in the news popularity prediction problem\n",
    "fig2, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.scatter(df['n_tokens_content'], df['shares'])\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_xlabel('Number of Words in Article', fontsize=14)\n",
    "ax1.set_ylabel('Number of shares', fontsize=14)\n",
    "\n",
    "ax2.scatter(df['log_n_tokens_content'], df['shares'])\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Log of Number of Words in Article', fontsize=14)\n",
    "ax2.set_ylabel('Number of shares', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-11\n",
    "# Visualizing the correlation between input and output in Yelp business review prediction\n",
    "# These two visualizatoins are there to show where the log transformation is more useful and it all depends on the type of data\n",
    "# and how it is scattered...\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.scatter(biz_df['review_count'], df['stars'])\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_xlabel('Review Count', fontsize=14)\n",
    "ax1.set_ylabel('Avg Star Rating', fontsize=14)\n",
    "\n",
    "ax2.scatter(df['log_review_count'], df['stars'])\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Log of Review Count', fontsize=14)\n",
    "ax2.set_ylabel('Avg Star Rating', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-12 \n",
    "# Power Transform\n",
    "# BOX-COX TRANSFORMATION\n",
    "# Box-Cox Transformation of Yelp Review Counts\n",
    "from scipy import stats\n",
    "# This transformation assumes that input_data is positive.\n",
    "# Check the min to make sure\n",
    "print(biz_df['review_count'].min())\n",
    "\n",
    "# Setting inp parameter lmbda to 0 gives us the log transform ( without constant offset)\n",
    "rc_log = stats.boxcox(biz_df['review_count'], lmbda=0)\n",
    "# By default, the scipy implementation of Box-Cox transform finds the lambda\n",
    "# parameter that will make the output the closest to a normal distribution\n",
    "rc_bc, bc_params = stats.boxcox(biz_df['review_count'])\n",
    "print(bc_params)\n",
    "\n",
    "# Visualizing the histogram of original, log transformed, and Box-Cox transformed counts\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
    "# Original review count histogram\n",
    "biz_df['review_count'].hist(ax=ax1, bins=100)\n",
    "ax1.set_yscale('log')\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_title('Review Counts Histogram', fontsize = 14)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Occurence', fontsize=14)\n",
    "\n",
    "# Review count after log transform\n",
    "biz_df['rc_log'].hist(ax=ax2, bins=100)\n",
    "ax2.set_yscale('log')\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_title('Log Transformed Counts Histogram', fontsize = 14)\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('Occurence', fontsize=14)\n",
    "\n",
    "# Review Counts after optimal Box-Cox transform\n",
    "biz_df['rc_bc'].hist(ax=ax3, bins=100)\n",
    "ax3.set_yscale('log')\n",
    "ax3.tick_params(labelsize=14)\n",
    "ax3.set_title('Box-Cox Transformed Counts Histogram', fontsize = 14)\n",
    "ax3.set_xlabel('')\n",
    "ax3.set_ylabel('Occurence', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-14\n",
    "# Probability Plots of original and transformed counts against the normal distribution\n",
    "fig2, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
    "prob1 = stats.probplot(biz_df['review_count'], dist=stats.norm, plot=ax1)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_title('ProbPlot against normal distribution')\n",
    "\n",
    "prob2 = stats.probplot(biz_df['rc_log'], dist=stats.norm, plot=ax2)\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_title('ProbPlot after log transform')\n",
    "\n",
    "prob3 = stats.probplot(biz_df['rc_bc'], dist=stats.norm, plot=ax3)\n",
    "ax3.set_xlabel('Theoretical Quantiles')\n",
    "ax3.set_title('ProbPlot after Box-Cox transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-15\n",
    "# Feature Scaling and Normalization\n",
    "# Feature Scaling Implementation\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preproc\n",
    "# Load data\n",
    "df = pd.read_csv(\"OnlineNewsPopularity.csv\", delimiter = ',')\n",
    "# Look at the original data- the number of words in an article\n",
    "print(df['n_tokens_content'].as_matrix())\n",
    "\n",
    "# Min-Max Scaling\n",
    "df['minmax'] = preproc.minmax_scale(df['n_tokens_content'])\n",
    "print(df['minmax'].as_matrix())\n",
    "\n",
    "# Standardization- some outputs can be -ve\n",
    "df['standardized'] = preproc.StandardScaler().fit_transform(df['n_tokens_content'])\n",
    "print(df['standardized'].as_matrix())\n",
    "\n",
    "# L-2 Normalization\n",
    "df['l2_normalized'] = preproc.normalize(df['n_tokens_content'], axis = 0)\n",
    "print(df['l2_normalized'].as_matrix())\n",
    "\n",
    "# Plotting Histograms of original and scaled data\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1)\n",
    "fig.tight_layout()\n",
    "\n",
    "df['n_tokens_count'].hist(ax=ax1, bins=100)\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_xlabel('Article Word Count', fontsize=14)\n",
    "ax1.set_ylabel('Number of Articles', fontsize=14)\n",
    "\n",
    "df['minmax'].hist(ax=ax2, bins=100)\n",
    "ax2.tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Min-Max Scaled Word Count', fontsize=14)\n",
    "ax2.set_ylabel('Number of Articles', fontsize=14)\n",
    "\n",
    "df['standardized'].hist(ax=ax3, bins=100)\n",
    "ax3.tick_params(labelsize=14)\n",
    "ax3.set_xlabel('Standardized Word Count', fontsize=14)\n",
    "ax3.set_ylabel('Number of Articles', fontsize=14)\n",
    "\n",
    "df['l2_normalized'].hist(ax=ax4, bins=100)\n",
    "ax4.tick_params(labelsize=14)\n",
    "ax4.set_xlabel('L-2 Normalized Word Count', fontsize=14)\n",
    "ax4.set_ylabel('Number of Articles', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-17\n",
    "# Interaction Features Prediction\n",
    "# Better method than taking just one feature into consideration for prediction...\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing as preproc\n",
    "\n",
    "# Assume df is a dataframe containing UCI Online News Popularity dataset\n",
    "print(df.columns)\n",
    "# Select the content-based features as singleton features in the model, skipping over the derived features\n",
    "features = []\n",
    "X = df[features]\n",
    "y = df['shares']\n",
    "\n",
    "# Create pairwise interaction features, skipping the constant bias term\n",
    "X2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X)\n",
    "print(X2.shape)\n",
    "\n",
    "# Create train/test sets for both feature sets\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X, X2, y, test_size=0.3, random_state=123)\n",
    "\n",
    "def evaluate_feature(X_train, X_test, y_train, y_test):\n",
    "    model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "    r_score = model.score(X_test, y_test)\n",
    "    return (model, r_score)\n",
    "\n",
    "(m1, r1) = evaluate_feature(X1_train, X1_test, y_train, y_test)\n",
    "(m2, r2) = evaluate_feature(X2_train, X2_test, y_train, y_test)\n",
    "\n",
    "print(\"R-Squared Score with singleton features: %0.5f\" % r1)\n",
    "print(\"R-Squared Score with pairwise features: %0.10f\" % r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
